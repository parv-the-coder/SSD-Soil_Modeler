{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒ± Comprehensive Spectral Soil Data Analysis\n",
    "\n",
    "This notebook provides an in-depth exploratory data analysis of the spectral soil dataset, including:\n",
    "- **Data Overview & Quality Assessment**\n",
    "- **Spectral Curve Analysis & Visualization** \n",
    "- **Preprocessing Methods Comparison**\n",
    "- **Feature Importance & Correlation Analysis**\n",
    "- **Model Performance Insights**\n",
    "\n",
    "## ðŸ“Š Dataset Information\n",
    "- **Wavelength Range**: 410-2490 nm (100 bands)\n",
    "- **Spectral Resolution**: ~21 nm\n",
    "- **Applications**: Soil property prediction from hyperspectral reflectance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Load data\n",
    "print(\"ðŸ“ Loading spectral soil dataset...\")\n",
    "data_path = '../data/spectra_with_target_T2.xls'\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv(data_path)  # Try CSV first (more reliable)\n",
    "    print(f\"âœ… Successfully loaded data from CSV format\")\n",
    "except:\n",
    "    try:\n",
    "        data = pd.read_excel(data_path, engine='xlrd')\n",
    "        print(f\"âœ… Successfully loaded data from Excel format\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        \n",
    "print(f\"ðŸ“Š Dataset shape: {data.shape[0]:,} samples Ã— {data.shape[1]:,} features\")\n",
    "print(f\"ðŸ“ˆ Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display basic info\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“‹ Data Quality Assessment\n",
    "print(\"ðŸ” DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Separate spectral data from target\n",
    "spectral_cols = [col for col in data.columns if col != 'target']\n",
    "target_col = 'target'\n",
    "\n",
    "print(f\"ðŸŽ¯ Target variable: '{target_col}'\")\n",
    "print(f\"ðŸ“¡ Spectral bands: {len(spectral_cols)} wavelengths ({spectral_cols[0]} - {spectral_cols[-1]} nm)\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nðŸ“Š BASIC STATISTICS\")\n",
    "print(\"-\" * 30)\n",
    "stats = data.describe()\n",
    "display(stats.round(4))\n",
    "\n",
    "# Missing values analysis\n",
    "missing_data = data.isnull().sum()\n",
    "if missing_data.sum() > 0:\n",
    "    print(f\"\\nâš ï¸  MISSING VALUES DETECTED\")\n",
    "    print(\"-\" * 30)\n",
    "    missing_summary = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    display(missing_summary)\n",
    "else:\n",
    "    print(f\"\\nâœ… NO MISSING VALUES DETECTED\")\n",
    "    print(\"All spectral data is complete!\")\n",
    "\n",
    "# Data type information\n",
    "print(f\"\\nðŸ·ï¸  DATA TYPES\")\n",
    "print(\"-\" * 15)\n",
    "print(data.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ Target Variable Analysis\n",
    "print(\"ðŸŽ¯ TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution histogram\n",
    "axes[0,0].hist(data[target_col], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].axvline(data[target_col].mean(), color='red', linestyle='--', label=f'Mean: {data[target_col].mean():.2f}')\n",
    "axes[0,0].axvline(data[target_col].median(), color='green', linestyle='--', label=f'Median: {data[target_col].median():.2f}')\n",
    "axes[0,0].set_title('Target Distribution')\n",
    "axes[0,0].set_xlabel('Target Value')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "box_plot = axes[0,1].boxplot(data[target_col], patch_artist=True)\n",
    "box_plot['boxes'][0].set_facecolor('lightcoral')\n",
    "axes[0,1].set_title('Target Box Plot')\n",
    "axes[0,1].set_ylabel('Target Value')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "from scipy import stats\n",
    "stats.probplot(data[target_col], dist=\"norm\", plot=axes[1,0])\n",
    "axes[1,0].set_title('Q-Q Plot (Normal Distribution Check)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Density plot with normal overlay\n",
    "axes[1,1].hist(data[target_col], bins=30, density=True, alpha=0.7, color='lightblue', label='Data')\n",
    "mu, sigma = data[target_col].mean(), data[target_col].std()\n",
    "x = np.linspace(data[target_col].min(), data[target_col].max(), 100)\n",
    "axes[1,1].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', label=f'Normal(Î¼={mu:.2f}, Ïƒ={sigma:.2f})')\n",
    "axes[1,1].set_title('Target Density vs Normal Distribution')\n",
    "axes[1,1].set_xlabel('Target Value')\n",
    "axes[1,1].set_ylabel('Density')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary\n",
    "print(f\"\\nðŸ“ˆ TARGET STATISTICS\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Mean: {data[target_col].mean():.4f}\")\n",
    "print(f\"Median: {data[target_col].median():.4f}\")\n",
    "print(f\"Std Dev: {data[target_col].std():.4f}\")\n",
    "print(f\"Min: {data[target_col].min():.4f}\")\n",
    "print(f\"Max: {data[target_col].max():.4f}\")\n",
    "print(f\"Range: {data[target_col].max() - data[target_col].min():.4f}\")\n",
    "print(f\"CV (Coefficient of Variation): {(data[target_col].std()/data[target_col].mean())*100:.2f}%\")\n",
    "\n",
    "# Skewness and Kurtosis\n",
    "skewness = stats.skew(data[target_col])\n",
    "kurtosis = stats.kurtosis(data[target_col])\n",
    "print(f\"Skewness: {skewness:.4f} {'(Right-skewed)' if skewness > 0 else '(Left-skewed)' if skewness < 0 else '(Symmetric)'}\")\n",
    "print(f\"Kurtosis: {kurtosis:.4f} {'(Heavy-tailed)' if kurtosis > 0 else '(Light-tailed)' if kurtosis < 0 else '(Normal-tailed)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“¡ Spectral Data Analysis\n",
    "print(\"ðŸ“¡ SPECTRAL CURVE ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Convert column names to numeric wavelengths for plotting\n",
    "wavelengths = np.array([float(col) for col in spectral_cols])\n",
    "spectral_data = data[spectral_cols]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Sample spectral curves\n",
    "n_samples = min(20, len(data))\n",
    "sample_indices = np.random.choice(len(data), n_samples, replace=False)\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, n_samples))\n",
    "\n",
    "for i, idx in enumerate(sample_indices[:10]):  # Show first 10 for clarity\n",
    "    axes[0,0].plot(wavelengths, spectral_data.iloc[idx], color=colors[i], alpha=0.7, linewidth=1)\n",
    "\n",
    "axes[0,0].set_title('Sample Spectral Reflectance Curves (Random 10 samples)')\n",
    "axes[0,0].set_xlabel('Wavelength (nm)')\n",
    "axes[0,0].set_ylabel('Reflectance')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Mean spectral curve with confidence intervals\n",
    "mean_spectrum = spectral_data.mean()\n",
    "std_spectrum = spectral_data.std()\n",
    "\n",
    "axes[0,1].plot(wavelengths, mean_spectrum, 'b-', linewidth=2, label='Mean')\n",
    "axes[0,1].fill_between(wavelengths, \n",
    "                      mean_spectrum - std_spectrum, \n",
    "                      mean_spectrum + std_spectrum, \n",
    "                      alpha=0.3, label='Â±1 Std Dev')\n",
    "axes[0,1].set_title('Mean Spectral Curve Â± Standard Deviation')\n",
    "axes[0,1].set_xlabel('Wavelength (nm)')\n",
    "axes[0,1].set_ylabel('Reflectance')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Spectral variability across wavelengths\n",
    "coefficient_of_variation = (std_spectrum / mean_spectrum) * 100\n",
    "\n",
    "axes[1,0].plot(wavelengths, coefficient_of_variation, 'g-', linewidth=2)\n",
    "axes[1,0].set_title('Coefficient of Variation Across Wavelengths')\n",
    "axes[1,0].set_xlabel('Wavelength (nm)')\n",
    "axes[1,0].set_ylabel('CV (%)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Mark regions of high variability\n",
    "high_var_threshold = coefficient_of_variation.quantile(0.9)\n",
    "high_var_regions = wavelengths[coefficient_of_variation > high_var_threshold]\n",
    "if len(high_var_regions) > 0:\n",
    "    axes[1,0].axhline(y=high_var_threshold, color='red', linestyle='--', \n",
    "                     label=f'90th percentile ({high_var_threshold:.1f}%)')\n",
    "    axes[1,0].legend()\n",
    "\n",
    "# 4. Spectral regions analysis\n",
    "# Divide spectrum into regions\n",
    "regions = {\n",
    "    'Visible (400-700nm)': (400, 700),\n",
    "    'NIR (700-1400nm)': (700, 1400), \n",
    "    'SWIR1 (1400-1900nm)': (1400, 1900),\n",
    "    'SWIR2 (1900-2500nm)': (1900, 2500)\n",
    "}\n",
    "\n",
    "region_stats = {}\n",
    "for region_name, (start, end) in regions.items():\n",
    "    region_mask = (wavelengths >= start) & (wavelengths <= end)\n",
    "    if region_mask.any():\n",
    "        region_data = spectral_data.iloc[:, region_mask]\n",
    "        region_stats[region_name] = {\n",
    "            'mean_reflectance': region_data.mean().mean(),\n",
    "            'std_reflectance': region_data.mean().std(),\n",
    "            'n_bands': region_mask.sum()\n",
    "        }\n",
    "\n",
    "# Plot region statistics\n",
    "region_names = list(region_stats.keys())\n",
    "means = [region_stats[r]['mean_reflectance'] for r in region_names]\n",
    "stds = [region_stats[r]['std_reflectance'] for r in region_names]\n",
    "\n",
    "x_pos = np.arange(len(region_names))\n",
    "bars = axes[1,1].bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color=['blue', 'green', 'orange', 'red'])\n",
    "axes[1,1].set_title('Mean Reflectance by Spectral Region')\n",
    "axes[1,1].set_xlabel('Spectral Region')\n",
    "axes[1,1].set_ylabel('Mean Reflectance')\n",
    "axes[1,1].set_xticks(x_pos)\n",
    "axes[1,1].set_xticklabels(region_names, rotation=45)\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add text annotations on bars\n",
    "for i, (bar, mean_val, n_bands) in enumerate(zip(bars, means, [region_stats[r]['n_bands'] for r in region_names])):\n",
    "    height = bar.get_height()\n",
    "    axes[1,1].text(bar.get_x() + bar.get_width()/2., height + stds[i] + 0.01,\n",
    "                  f'{n_bands} bands', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print spectral statistics\n",
    "print(f\"ðŸ“Š SPECTRAL DATA SUMMARY\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Wavelength range: {wavelengths.min():.0f} - {wavelengths.max():.0f} nm\")\n",
    "print(f\"Number of spectral bands: {len(wavelengths)}\")\n",
    "print(f\"Average spectral resolution: {np.mean(np.diff(wavelengths)):.1f} nm\")\n",
    "print(f\"Overall mean reflectance: {spectral_data.mean().mean():.4f}\")\n",
    "print(f\"Overall std reflectance: {spectral_data.std().mean():.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ” BY SPECTRAL REGION:\")\n",
    "for region, stats in region_stats.items():\n",
    "    print(f\"{region}: {stats['n_bands']} bands, mean={stats['mean_reflectance']:.4f}\")\n",
    "\n",
    "# Identify most/least variable bands\n",
    "most_variable = coefficient_of_variation.nlargest(5)\n",
    "least_variable = coefficient_of_variation.nsmallest(5)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ MOST VARIABLE WAVELENGTHS:\")\n",
    "for wl, cv in most_variable.items():\n",
    "    print(f\"  {wl} nm: {cv:.2f}% CV\")\n",
    "    \n",
    "print(f\"\\nðŸ“‰ LEAST VARIABLE WAVELENGTHS:\")\n",
    "for wl, cv in least_variable.items():\n",
    "    print(f\"  {wl} nm: {cv:.2f}% CV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”— Correlation Analysis and Feature Importance\n",
    "print(\"ðŸ”— CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate correlation matrix (sample subset for memory efficiency)\n",
    "sample_size = min(1000, len(data))\n",
    "sample_data = data.sample(n=sample_size, random_state=42)\n",
    "corr_matrix = sample_data.corr()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Correlation heatmap (focused on target correlations)\n",
    "target_corr = corr_matrix[target_col].drop(target_col)\n",
    "# Get top positive and negative correlations\n",
    "top_pos = target_corr.nlargest(20)\n",
    "top_neg = target_corr.nsmallest(20) \n",
    "important_features = pd.concat([top_pos, top_neg]).index.tolist()\n",
    "\n",
    "# Create subset correlation matrix for visualization\n",
    "subset_corr = corr_matrix.loc[important_features + [target_col], important_features + [target_col]]\n",
    "\n",
    "sns.heatmap(subset_corr, annot=False, cmap='RdBu_r', center=0, \n",
    "            cbar_kws={'label': 'Correlation Coefficient'}, ax=axes[0,0])\n",
    "axes[0,0].set_title('Correlation Heatmap (Top Correlated Features)')\n",
    "\n",
    "# 2. Target correlation across wavelengths\n",
    "wavelengths = np.array([float(col) for col in spectral_cols])\n",
    "target_corr_values = [target_corr[col] if col in target_corr.index else 0 for col in spectral_cols]\n",
    "\n",
    "axes[0,1].plot(wavelengths, target_corr_values, 'b-', linewidth=2)\n",
    "axes[0,1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0,1].axhline(y=0.3, color='red', linestyle='--', alpha=0.5, label='Strong positive (>0.3)')\n",
    "axes[0,1].axhline(y=-0.3, color='red', linestyle='--', alpha=0.5, label='Strong negative (<-0.3)')\n",
    "axes[0,1].set_title('Correlation with Target Across Wavelengths')\n",
    "axes[0,1].set_xlabel('Wavelength (nm)')\n",
    "axes[0,1].set_ylabel('Correlation Coefficient')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Top correlating wavelengths\n",
    "top_corr_abs = target_corr.abs().nlargest(15)\n",
    "colors = ['red' if target_corr[idx] < 0 else 'blue' for idx in top_corr_abs.index]\n",
    "\n",
    "axes[1,0].barh(range(len(top_corr_abs)), [target_corr[idx] for idx in top_corr_abs.index], color=colors, alpha=0.7)\n",
    "axes[1,0].set_yticks(range(len(top_corr_abs)))\n",
    "axes[1,0].set_yticklabels([f'{idx} nm' for idx in top_corr_abs.index])\n",
    "axes[1,0].set_title('Top 15 Wavelengths by Absolute Correlation')\n",
    "axes[1,0].set_xlabel('Correlation Coefficient')\n",
    "axes[1,0].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Correlation distribution\n",
    "all_corrs = target_corr.values\n",
    "axes[1,1].hist(all_corrs, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1,1].axvline(np.mean(all_corrs), color='red', linestyle='--', label=f'Mean: {np.mean(all_corrs):.3f}')\n",
    "axes[1,1].axvline(np.median(all_corrs), color='green', linestyle='--', label=f'Median: {np.median(all_corrs):.3f}')\n",
    "axes[1,1].set_title('Distribution of Correlations with Target')\n",
    "axes[1,1].set_xlabel('Correlation Coefficient')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation statistics\n",
    "print(f\"ðŸ“Š CORRELATION STATISTICS\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"Highest positive correlation: {target_corr.max():.4f} at {target_corr.idxmax()} nm\")\n",
    "print(f\"Highest negative correlation: {target_corr.min():.4f} at {target_corr.idxmin()} nm\")\n",
    "print(f\"Mean absolute correlation: {target_corr.abs().mean():.4f}\")\n",
    "print(f\"Median absolute correlation: {target_corr.abs().median():.4f}\")\n",
    "\n",
    "# Strong correlations\n",
    "strong_pos = target_corr[target_corr > 0.3]\n",
    "strong_neg = target_corr[target_corr < -0.3]\n",
    "print(f\"\\nðŸ”´ Strong positive correlations (>0.3): {len(strong_pos)}\")\n",
    "if len(strong_pos) > 0:\n",
    "    print(\"Top 5:\", strong_pos.nlargest(5).round(4).to_dict())\n",
    "\n",
    "print(f\"\\nðŸ”µ Strong negative correlations (<-0.3): {len(strong_neg)}\")\n",
    "if len(strong_neg) > 0:\n",
    "    print(\"Top 5:\", strong_neg.nsmallest(5).round(4).to_dict())\n",
    "\n",
    "# Wavelength regions with high correlation\n",
    "print(f\"\\nðŸŒˆ CORRELATION BY SPECTRAL REGION:\")\n",
    "for region_name, (start, end) in regions.items():\n",
    "    region_mask = (wavelengths >= start) & (wavelengths <= end)\n",
    "    if region_mask.any():\n",
    "        region_wavelengths = wavelengths[region_mask]\n",
    "        region_corrs = [target_corr_values[i] for i, wl in enumerate(wavelengths) if wl in region_wavelengths]\n",
    "        if region_corrs:\n",
    "            mean_corr = np.mean(np.abs(region_corrs))\n",
    "            print(f\"  {region_name}: Mean |correlation| = {mean_corr:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ FEATURE SELECTION INSIGHTS:\")\n",
    "print(\"-\" * 35)\n",
    "high_corr_features = target_corr.abs().nlargest(50).index.tolist()\n",
    "print(f\"â€¢ Top 50 features by correlation could be good candidates\")\n",
    "print(f\"â€¢ Consider wavelengths: {', '.join([f'{wl}nm' for wl in high_corr_features[:10]])}...\")\n",
    "print(f\"â€¢ Spectral regions to focus on: Areas with consistently high correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¬ Preprocessing Methods Comparison\n",
    "print(\"ðŸ”¬ PREPROCESSING METHODS COMPARISON\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Define preprocessing functions\n",
    "def reflectance(x):\n",
    "    \"\"\"Identity function - no preprocessing\"\"\"\n",
    "    return x.copy()\n",
    "\n",
    "def absorbance(x):\n",
    "    \"\"\"Convert reflectance to absorbance: -log10(R)\"\"\"\n",
    "    return -np.log10(x.replace(0, 1e-6))\n",
    "\n",
    "def continuum_removal(x):\n",
    "    \"\"\"Continuum removal: divide by row maximum\"\"\"\n",
    "    row_max = x.max(axis=1).replace(0, 1e-6)\n",
    "    return x.div(row_max, axis=0)\n",
    "\n",
    "# Apply preprocessing to sample data\n",
    "sample_spectrum = spectral_data.iloc[0]\n",
    "wavelengths = np.array([float(col) for col in spectral_cols])\n",
    "\n",
    "# Create preprocessed versions\n",
    "original = sample_spectrum\n",
    "absorbed = absorbance(sample_spectrum.to_frame().T).iloc[0]\n",
    "continuum_removed = continuum_removal(sample_spectrum.to_frame().T).iloc[0]\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original spectrum\n",
    "axes[0,0].plot(wavelengths, original, 'b-', linewidth=2)\n",
    "axes[0,0].set_title('Original Reflectance')\n",
    "axes[0,0].set_xlabel('Wavelength (nm)')\n",
    "axes[0,0].set_ylabel('Reflectance')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Absorbance\n",
    "axes[0,1].plot(wavelengths, absorbed, 'r-', linewidth=2)\n",
    "axes[0,1].set_title('Absorbance (-log10(R))')\n",
    "axes[0,1].set_xlabel('Wavelength (nm)')\n",
    "axes[0,1].set_ylabel('Absorbance')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Continuum removal\n",
    "axes[1,0].plot(wavelengths, continuum_removed, 'g-', linewidth=2)\n",
    "axes[1,0].set_title('Continuum Removal (R/R_max)')\n",
    "axes[1,0].set_xlabel('Wavelength (nm)')\n",
    "axes[1,0].set_ylabel('Normalized Reflectance')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# All together for comparison\n",
    "axes[1,1].plot(wavelengths, original/original.max(), 'b-', label='Reflectance (normalized)', alpha=0.7)\n",
    "axes[1,1].plot(wavelengths, absorbed/absorbed.max(), 'r-', label='Absorbance (normalized)', alpha=0.7)\n",
    "axes[1,1].plot(wavelengths, continuum_removed, 'g-', label='Continuum Removal', alpha=0.7)\n",
    "axes[1,1].set_title('Preprocessing Methods Comparison')\n",
    "axes[1,1].set_xlabel('Wavelength (nm)')\n",
    "axes[1,1].set_ylabel('Normalized Response')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“‹ PREPROCESSING METHOD CHARACTERISTICS:\")\n",
    "print(\"-\" * 45)\n",
    "print(\"ðŸ”µ Reflectance (Original):\")\n",
    "print(\"   â€¢ Raw spectral values\")\n",
    "print(\"   â€¢ Maintains absolute reflectance levels\")\n",
    "print(\"   â€¢ Good for: General spectral analysis\")\n",
    "\n",
    "print(\"\\nðŸ”´ Absorbance (-log10(R)):\")\n",
    "print(\"   â€¢ Emphasizes absorption features\")\n",
    "print(\"   â€¢ Linear relationship with concentration (Beer's Law)\")\n",
    "print(\"   â€¢ Good for: Chemical constituent analysis\")\n",
    "\n",
    "print(\"\\nðŸŸ¢ Continuum Removal:\")\n",
    "print(\"   â€¢ Normalizes by spectrum maximum\")\n",
    "print(\"   â€¢ Reduces baseline effects\")\n",
    "print(\"   â€¢ Good for: Shape-based analysis, reducing illumination effects\")\n",
    "\n",
    "# Calculate correlation with target for each preprocessing method\n",
    "print(f\"\\nðŸŽ¯ CORRELATION WITH TARGET BY PREPROCESSING:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Sample a subset for faster computation\n",
    "sample_size = min(500, len(data))\n",
    "sample_data = data.sample(n=sample_size, random_state=42)\n",
    "\n",
    "for method_name, method_func in [('Reflectance', reflectance), \n",
    "                                ('Absorbance', absorbance), \n",
    "                                ('Continuum Removal', continuum_removal)]:\n",
    "    # Apply preprocessing\n",
    "    preprocessed = method_func(sample_data[spectral_cols])\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    target_subset = sample_data[target_col]\n",
    "    correlations = []\n",
    "    for col in preprocessed.columns:\n",
    "        corr = np.corrcoef(preprocessed[col], target_subset)[0,1]\n",
    "        correlations.append(abs(corr))\n",
    "    \n",
    "    mean_abs_corr = np.mean(correlations)\n",
    "    max_abs_corr = np.max(correlations)\n",
    "    \n",
    "    print(f\"{method_name:.<20} Mean |r|: {mean_abs_corr:.4f}, Max |r|: {max_abs_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ† Model Performance Analysis\n",
    "print(\"ðŸ† MODEL PERFORMANCE INSIGHTS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load model performance logs if available\n",
    "model_results = {}\n",
    "models_dir = Path('../models')\n",
    "\n",
    "if models_dir.exists():\n",
    "    for i in range(1, 6):\n",
    "        log_file = models_dir / f'best_model_T{i}_log.txt'\n",
    "        if log_file.exists():\n",
    "            with open(log_file, 'r') as f:\n",
    "                content = f.read()\n",
    "                # Extract performance metrics using regex\n",
    "                import re\n",
    "                r2_matches = re.findall(r'RÂ²:\\s*([\\d.]+)', content)\n",
    "                rmse_matches = re.findall(r'RMSE:\\s*([\\d.]+)', content)\n",
    "                \n",
    "                if r2_matches and rmse_matches:\n",
    "                    best_r2 = max([float(r2) for r2 in r2_matches])\n",
    "                    best_rmse = min([float(rmse) for rmse in rmse_matches])\n",
    "                    \n",
    "                    # Extract best pipeline name\n",
    "                    pipeline_match = re.search(r'Best pipeline: (.+?) with', content)\n",
    "                    best_pipeline = pipeline_match.group(1) if pipeline_match else 'Unknown'\n",
    "                    \n",
    "                    model_results[f'T{i}'] = {\n",
    "                        'RÂ²': best_r2,\n",
    "                        'RMSE': best_rmse, \n",
    "                        'Pipeline': best_pipeline\n",
    "                    }\n",
    "\n",
    "if model_results:\n",
    "    # Create comparison plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    models = list(model_results.keys())\n",
    "    r2_scores = [model_results[m]['RÂ²'] for m in models]\n",
    "    rmse_scores = [model_results[m]['RMSE'] for m in models]\n",
    "    pipelines = [model_results[m]['Pipeline'] for m in models]\n",
    "    \n",
    "    # RÂ² comparison\n",
    "    bars1 = axes[0,0].bar(models, r2_scores, color='skyblue', alpha=0.8)\n",
    "    axes[0,0].set_title('Model Performance: RÂ² Scores')\n",
    "    axes[0,0].set_ylabel('RÂ² Score')\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars1, r2_scores):\n",
    "        height = bar.get_height()\n",
    "        axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # RMSE comparison  \n",
    "    bars2 = axes[0,1].bar(models, rmse_scores, color='lightcoral', alpha=0.8)\n",
    "    axes[0,1].set_title('Model Performance: RMSE Scores')\n",
    "    axes[0,1].set_ylabel('RMSE')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, score in zip(bars2, rmse_scores):\n",
    "        height = bar.get_height()\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2., height + max(rmse_scores)*0.01,\n",
    "                      f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Pipeline distribution\n",
    "    unique_pipelines = list(set(pipelines))\n",
    "    pipeline_counts = [pipelines.count(p) for p in unique_pipelines]\n",
    "    \n",
    "    axes[1,0].pie(pipeline_counts, labels=unique_pipelines, autopct='%1.0f%%', \n",
    "                 colors=plt.cm.Set3(np.linspace(0, 1, len(unique_pipelines))))\n",
    "    axes[1,0].set_title('Best Pipeline Distribution')\n",
    "    \n",
    "    # Performance vs Model complexity (placeholder)\n",
    "    axes[1,1].scatter(r2_scores, rmse_scores, c=range(len(models)), \n",
    "                     cmap='viridis', s=100, alpha=0.7)\n",
    "    for i, model in enumerate(models):\n",
    "        axes[1,1].annotate(model, (r2_scores[i], rmse_scores[i]), \n",
    "                          xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1,1].set_xlabel('RÂ² Score')\n",
    "    axes[1,1].set_ylabel('RMSE')\n",
    "    axes[1,1].set_title('RÂ² vs RMSE Trade-off')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"ðŸ“Š MODEL PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 50)\n",
    "    performance_df = pd.DataFrame(model_results).T\n",
    "    display(performance_df.round(4))\n",
    "    \n",
    "    # Best performing model\n",
    "    best_model = max(model_results.keys(), key=lambda x: model_results[x]['RÂ²'])\n",
    "    print(f\"\\nðŸ¥‡ BEST PERFORMING MODEL:\")\n",
    "    print(f\"   Model: {best_model}\")\n",
    "    print(f\"   RÂ²: {model_results[best_model]['RÂ²']:.4f}\")\n",
    "    print(f\"   RMSE: {model_results[best_model]['RMSE']:.4f}\")\n",
    "    print(f\"   Pipeline: {model_results[best_model]['Pipeline']}\")\n",
    "    \n",
    "    # Performance insights\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ OVERALL INSIGHTS:\")\n",
    "    print(f\"   Average RÂ²: {mean_r2:.4f}\")\n",
    "    print(f\"   Average RMSE: {mean_rmse:.4f}\")\n",
    "    print(f\"   RÂ² Range: {min(r2_scores):.4f} - {max(r2_scores):.4f}\")\n",
    "    print(f\"   Most common pipeline: {max(set(pipelines), key=pipelines.count)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No model performance logs found.\")\n",
    "    print(\"ðŸ’¡ Train models first using the Streamlit app to see performance analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Key Findings & Recommendations\n",
    "\n",
    "## ðŸ“Š Summary of Analysis\n",
    "\n",
    "### ðŸ” Data Quality\n",
    "- âœ… **Complete Dataset**: No missing values detected\n",
    "- ðŸ“ **Spectral Coverage**: 100 bands from 410-2490 nm (~21 nm resolution)\n",
    "- ðŸ“ˆ **Sample Size**: Sufficient for robust model training\n",
    "\n",
    "### ðŸ“¡ Spectral Characteristics\n",
    "- **Most Variable Regions**: Identified wavelengths with highest coefficient of variation\n",
    "- **Stable Regions**: Bands with consistent measurements across samples\n",
    "- **Spectral Patterns**: Clear differences between visible, NIR, and SWIR regions\n",
    "\n",
    "### ðŸ”— Feature-Target Relationships\n",
    "- **Strong Correlations**: Found wavelengths with |r| > 0.3 with target\n",
    "- **Optimal Bands**: Identified top spectral features for prediction\n",
    "- **Regional Patterns**: Some spectral regions show consistently higher correlation\n",
    "\n",
    "### ðŸ”¬ Preprocessing Impact\n",
    "- **Continuum Removal**: Best for shape-based analysis and baseline correction\n",
    "- **Absorbance**: Effective for chemical constituent analysis\n",
    "- **Reflectance**: Good baseline for general spectral modeling\n",
    "\n",
    "## ðŸš€ Recommendations\n",
    "\n",
    "### ðŸ“ˆ Model Optimization\n",
    "1. **Focus on High-Correlation Bands**: Use top 50-100 features identified in correlation analysis\n",
    "2. **Test Preprocessing Combinations**: Compare performance across all three preprocessing methods\n",
    "3. **Cross-Validation**: Ensure robust validation across different data splits\n",
    "\n",
    "### ðŸŽ¯ Feature Engineering\n",
    "1. **Spectral Indices**: Calculate vegetation/soil indices from key wavelength combinations\n",
    "2. **Derivative Features**: Use first/second derivatives to enhance absorption features\n",
    "3. **Principal Components**: Consider PCA for dimensionality reduction while preserving variance\n",
    "\n",
    "### ðŸ”§ Technical Improvements\n",
    "1. **Pipeline Optimization**: Automate preprocessing selection based on correlation analysis\n",
    "2. **Ensemble Methods**: Combine predictions from multiple preprocessing approaches\n",
    "3. **Hyperparameter Tuning**: Use GridSearch/RandomSearch for optimal model parameters\n",
    "\n",
    "### ðŸ“Š Deployment Strategy\n",
    "1. **Model Validation**: Test on independent datasets when available\n",
    "2. **Uncertainty Quantification**: Implement prediction confidence intervals\n",
    "3. **Performance Monitoring**: Track model performance over time\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ This analysis provides the foundation for building robust spectral soil prediction models!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
